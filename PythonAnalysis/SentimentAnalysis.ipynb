{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import flask\n",
    "from flask import Flask, request, jsonify\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"sentiment_analysis.csv\")\n",
    "\n",
    "# Text cleaning function\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "sentiment_mapping = {'positive': 1, 'negative': 0}\n",
    "df = df[df['sentiment'].isin(sentiment_mapping)]\n",
    "df['sentiment'] = df['sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Train-test split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['clean_text'], df['sentiment'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize data\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Custom Dataset class\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}, torch.tensor(self.labels.iloc[idx])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Load BERT model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "def train_model():\n",
    "    model.train()\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "train_model()\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"bert_sentiment_model.pth\")\n",
    "\n",
    "# Load the trained model for inference\n",
    "model.load_state_dict(torch.load(\"bert_sentiment_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Function to predict sentiment\n",
    "def predict_sentiment(text):\n",
    "    cleaned_text = clean_text(text)\n",
    "    encoding = tokenizer(cleaned_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    encoding = {key: val.to(device) for key, val in encoding.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**encoding)\n",
    "        prediction = torch.argmax(output.logits).item()\n",
    "    \n",
    "    return \"Positive\" if prediction == 1 else \"Negative\"\n",
    "\n",
    "# Flask API for chatbot deployment\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/chatbot\", methods=[\"POST\"])\n",
    "def chatbot():\n",
    "    data = request.json\n",
    "    user_input = data.get(\"message\", \"\")\n",
    "\n",
    "    if not user_input:\n",
    "        return jsonify({\"response\": \"Please enter a message.\"})\n",
    "\n",
    "    sentiment = predict_sentiment(user_input)\n",
    "    response_text = \"I see you're feeling positive! ðŸ˜Š\" if sentiment == \"Positive\" else \"Oh no! That seems negative. ðŸ˜”\"\n",
    "    \n",
    "    return jsonify({\"response\": response_text, \"sentiment\": sentiment})\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5000, debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
